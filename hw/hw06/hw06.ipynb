{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65113060",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c8ae6",
   "metadata": {},
   "source": [
    "# üé≤ Homework 6: Probability (and a little bit of gradient descent)\n",
    "## Due Date: Friday, November 7th, 11:59 PM PDT\n",
    "\n",
    "**‚ÄºÔ∏è Note: There is also a written portion of this HW! Be sure to complete both this notebook and the written assignment.**\n",
    "\n",
    "You must submit this assignment to Pensieve by the on-time deadline, Friday, November 7th, 11:59 PM PDT. Please read the syllabus for the Slip Day policy. No late submissions beyond what is outlined in the Slip Day policy will be accepted. **We strongly encourage you to submit your work to Pensieve several hours before the stated deadline.** This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance.\n",
    "\n",
    "Please read the instructions carefully when submitting your work to Pensieve. \n",
    "\n",
    "This is part of a two-part assignment. After completing this part (\"Homework 6 Coding\"), also remember to complete the \"Homework 6 (Statistics)\" assignment, which will be about deriving the mathematical aspects of random variables and estimators to help you in upcoming statistics courses during your time at Berkeley.\n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d357f",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38c5ac",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this homework, we will explore a dataset of the 2022 U.S. News & World Report rankings for liberal arts colleges.\n",
    "The dataset in this HW is a simplified version of the [original data source](https://andyreiter.com/datasets/). (Note: we are using the 2022 version, not the updated 2026 version)\n",
    "\n",
    "While working on this notebook, you will gain experience with the following:\n",
    "* Bootstrap sampling,\n",
    "* The bias-variance tradeoff and decomposition,\n",
    "* Multicollinearity in features, and\n",
    "* Gradient Descent.\n",
    "\n",
    "## Grading\n",
    "Grading is broken down into auto-graded answers and free responses. \n",
    "\n",
    "For auto-graded answers, the results of your code are compared to provided and/or hidden tests.\n",
    "\n",
    "For free response, readers will evaluate how well you answered the question and/or fulfilled the requirements of the question. \n",
    "\n",
    "### Score breakdown\n",
    "\n",
    "Question | Manual | Points\n",
    "--- |---| ---\n",
    "1a| No | 2\n",
    "1b| No | 3\n",
    "1c| Yes | 3\n",
    "1d| Yes | 2\n",
    "1e| Yes | 1\n",
    "2a| No | 3\n",
    "2b| Yes |3\n",
    "2c| Yes | 1\n",
    "3a| No | 2\n",
    "3b| No | 2\n",
    "3c| No | 2\n",
    "3d(i)| Yes | 1.5\n",
    "3d(ii)| Yes | 1.5\n",
    "4a(i)| Yes | 2\n",
    "4a(ii)| Yes | 2\n",
    "4a(iii)| Yes | 2\n",
    "4b| Yes | 2\n",
    "5a| No | 0\n",
    "5b| Yes | 0\n",
    "Total | 12 | 35\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import scipy.stats\n",
    "from hw06_utils import visualize_gradient_descent, plot_function, generate_X, generate_Y\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae55c9",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## üìà Question 1: Exploratory Data Analysis\n",
    "\n",
    "Let's perform some initial EDA to identify trends and patterns in the college ranking dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data; no further action is needed.\n",
    "college_data = pd.read_csv('data/2022_college_rankings.csv')\n",
    "college_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130f947",
   "metadata": {},
   "source": [
    "The granularity of the dataset is an individual **college**; each row includes metrics, ratings, and features of **one college in the U.S**.\n",
    "\n",
    "Here are the columns in the dataset:\n",
    "* `Institution`: The name of the college\n",
    "* `Type`: Whether the college is `'private'` or `'public'`\n",
    "* `Overall Score (0-100)`: The score computed by U.S. News & World Report to generate the ranking. **This is our outcome variable.**\n",
    "* The remaining columns contain features that are related to each college's overall score. \n",
    "\n",
    "We can use `college_data.info()` and `college_data.describe()` to see various statistics about the features of the provided college ranking data. Do any particular statistics stand out to you? Which might be useful when modeling?\n",
    "\n",
    "**Note:** This isn't a question, so it's not worth any points. This is just food for thought as you start to explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3789124",
   "metadata": {},
   "outputs": [],
   "source": [
    "college_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "college_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3bcf2-a56e-4d79-b126-7fbb5bdfbf2b",
   "metadata": {},
   "source": [
    "You may have observed that several features in the dataset are not shown when viewing with the `describe()` method. This is because those features are non-numeric. For example, some features are in the string format `\"XX%\"`.\n",
    "\n",
    "Before we can fit a linear model accounting for all features in the dataset, we have to convert all features to a numeric representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06776ec8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "Create a new `DataFrame` `cleaned_college_data` with the following changes:\n",
    "\n",
    "- For each column containing at least one missing value, add a new Boolean column that is `True` when the column value is missing, and `False` when the column value is not missing.\n",
    "    - For example, if there were a column containing the values `1, 2, NaN`, we would add a column containing the values `False, False, True`.\n",
    "    - If the original column name is `COLUMN`, the added column name should be `COLUMN_missing`. \n",
    "\n",
    "**Hint**: Use a `for` loop to iterate over the existing column names. You may find the `any()` function [(documentation)](https://docs.python.org/3/library/functions.html#any) useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903f696",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "cleaned_college_data = college_data.copy()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0d739",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787a6f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "Now, we will convert all non-numeric columns of `cleaned_college_data` into numbers. Additionally, we will create some new columns to encode additional information.\n",
    "\n",
    "To do so, complete the following tasks **in the order they are listed**:\n",
    "1. Consider the columns that contain the `'%'` symbol in any entry. Convert these columns from a string representation into the floating point number preceding `'%'`.\n",
    "    - For example, the string `\"96.4%\"` should be converted into floating point number `96.4`.\n",
    "2. Create a column called `\"Is Public\"` where the value is `1` if the row's college is public and `0` if the row's college is private.\n",
    "3. Create a column called `\"Is Private\"` where the value is `1` if the row's college is private and `0` if the row's college is public.\n",
    "4. Remove the `\"Type\"` and `\"Institution\"` columns.\n",
    "5. Replace all `NaN` values in the data with the mean of their respective column.\n",
    "\n",
    "**Hint**:\n",
    "- `pd.Series.fillna()` [(documentation)](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) may come in handy.\n",
    "- You can select all string columns in a `DataFrame` object `df` with `df.select_dtypes(include=['object'])`[(documentation)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html). Recall that strings are of the `object` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a3139",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "cleaned_college_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb1814",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c56e7e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "Linear models are sensitive to multicollinearity among the columns of the design matrix. So, let's determine the extent of multicollinearity in the college rankings dataset.\n",
    "\n",
    "In the following cell, we provide you with `arranged_cleaned_college_data`, which is a copy of `cleaned_college_data` containing just a subset of the columns arranged in a particular order. \n",
    "\n",
    "Create a visualization that shows the pairwise correlation between each combination of columns in `arranged_cleaned_college_data`. \n",
    "\n",
    "- For 2-D visualizations, consider the `sns.heatmap()` [documentation](https://seaborn.pydata.org/generated/seaborn.heatmap.html). \n",
    "\n",
    "- For full credit, title your plot, and set `annot=True`. This makes the plot easier to interpret.\n",
    "\n",
    "- You may find your plot easier to read with a different color scale. For example, try including `cmap=\"coolwarm\"` inside of `sns.heatmap()`.\n",
    "\n",
    "**Hint**: Your plot should show $10 \\times 10$ values corresponding to the [pairwise correlations](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) of the selected columns in `arranged_cleaned_college_data`:\n",
    "```\n",
    "['Overall Score (0-100)', \n",
    "'Peer Assessment Score (1-5)', \n",
    "'Predicted 6yr graduation rate',\n",
    "'Actual 6yr graduation rate', \n",
    "'Graduation and retention rank', \n",
    "'Student Excellence rank',\n",
    "'Acceptance rate', \n",
    "'Financial resources rank', \n",
    "'Is Public', \n",
    "'Is Private']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04ece9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Running this cell helps you get a subset of cleaned_college_data with the above columns\n",
    "arranged_cleaned_college_data = cleaned_college_data[\n",
    "    ['Overall Score (0-100)', \n",
    "     'Peer Assessment Score (1-5)', \n",
    "     'Predicted 6yr graduation rate',\n",
    "     'Actual 6yr graduation rate', \n",
    "     'Graduation and retention rank', \n",
    "     'Student Excellence rank',\n",
    "     'Acceptance rate', \n",
    "     'Financial resources rank', \n",
    "     'Is Public', \n",
    "     'Is Private']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56260b5b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Your output figure should look similar to the following example:\n",
    "\n",
    "<center><img src = \"images/heatmap.png\" width = \"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7ca49",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a25b54",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1d\n",
    "\n",
    "Do you notice any patterns in the plot from part (c)? What might explain these patterns? Comment and hypothesize on at least two patterns you notice.\n",
    "\n",
    "Here are some example questions to ponder:\n",
    "\n",
    "1. Why do some feature-pairs have correlations of $\\pm 1$? Is this a problem?\n",
    "2. What does the correlation between pairs of features (i.e., graduation-related statistics) look like? Is the magnitude of any of the correlations problematically close to 1?\n",
    "3. Are any features particularly strong predictors of the outcome? If so, why do you think this is the case?\n",
    "4. Do any features seem potentially redundant? In other words, do you suspect that any features provide similar information about the outcome as other features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b5aaa",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800eded6-448a-4837-99da-ce6174dc3ca9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1e\n",
    "If we tried to fit a linear regression model with an intercept term using all features in `cleaned_college_data`, we might run into some problems when fitting our model.\n",
    "The Data 100 staff suggests that we perform the following operation to the `DataFrame` before fitting a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ae980",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# You must run this cell to achieve pass the public tests for later questions.\n",
    "cleaned_college_data = cleaned_college_data.drop('Is Private', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8c06b-550d-444e-a178-1698ebe206df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Describe the reasoning behind this operation. What problem(s) do we avoid by removing the `Is Private` column from the model fitting process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1628a10",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079491b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## üèóÔ∏è Question 2: Constructing a Preliminary College Ranking Model\n",
    "\n",
    "In this question, you will fit a linear regression model to predict the U.S. News liberal arts college scores based on college metrics. Later, we will investigate the bias, variance, and observational noise of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ef547",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Train an `sklearn` OLS model that predicts each college's overall score using all of the other columns in `cleaned_college_data`. Include an intercept term in the model.\n",
    "\n",
    "Use `train_test_split()` [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to evaluate your model's RMSE on a held-out test set containing 33% of the college data; call the resulting splits `X_train`, `X_test`, `Y_train`, and `Y_test`.\n",
    "\n",
    "You can instantiate your linear regression model using `lm.LinearRegression` [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "**Note**: To pass the autograder, make sure to include `random_state=100` in your call to `train_test_split()` to generate a reproducible data split ([documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525c32c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create train/test sets\n",
    "X = ...\n",
    "Y = ...\n",
    "X_train, X_test, Y_train, Y_test = ...\n",
    "\n",
    "# Fit the linear model and make predictions (you will need multiple lines)\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# Compute RMSE on train and test sets\n",
    "train_rmse_cpc = ...\n",
    "test_rmse_cpc = ...\n",
    "\n",
    "# Ignore these 2 lines of code, they are for later\n",
    "X_original = X.copy()\n",
    "Y_original = Y.copy()\n",
    "\n",
    "train_rmse_cpc, test_rmse_cpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03272c1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7823ad",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Let's visualize the model performance from part 2(a). Plot the following:\n",
    "1. The observed values vs. the predicted values on the test set.\n",
    "2. The residuals plot. Recall that for multiple linear regression, we plot the residuals against the predicted values.\n",
    "\n",
    "**In both plots, the predicted values should be on the x-axis.**\n",
    "\n",
    "**Note:**\n",
    "* For a full-credit solution, you should use `plt.subplot()` ([documentation](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html)) so that you can view both visualizations side-by-side. \n",
    "    * The method `plt.subplot({# of rows}{# of cols}{index of plot})` sets the plottable area to the `index` of a `# rows` by `# cols` grid. \n",
    "    * For example, `plt.subplot(121)` sets the plottable area to the first index of a 1x2 plot grid. Calling `Matplotlib` and `Seaborn` functions will plot on the first index. When you're ready to start plotting on the second index, run `plt.subplot(122)`.\n",
    "* **Remember to add a guiding line to both plots where $\\hat{Y} = Y$, i.e., where the residual is 0**.\n",
    "    * `plt.plot()`[(documentation)](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) and `plt.axhline()`[(documentation)](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axhline.html) might be helpful here! \n",
    "* Make sure to add descriptive titles and axis labels.\n",
    "* To avoid distorted aspect ratios, ensure the limits of the x-axes for both plots are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45870d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))      # do not change this line\n",
    "plt.subplot(121)                # do not change this line\n",
    "# 1. plot observations vs. predictions\n",
    "...\n",
    "\n",
    "plt.subplot(122)               # do not change this line\n",
    "# 2. plot residual plot\n",
    "...\n",
    "\n",
    "plt.tight_layout()             # do not change this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185dd56",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2c\n",
    "\n",
    "Describe what the plots in part (b) indicate about this linear model. In particular, are the predictions good, and do the residuals appear uncorrelated with the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264dc31e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0bb3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## ü§π Question 3: Confidence Intervals and Regression Coefficients\n",
    "\n",
    "We'll next analyze the statistical significance of coefficients in our linear model. Specifically, we will use bootstrapping to get $95\\%$ confidence intervals on the fitted parameters. Each step of bootstrapping corresponds to one part of this question:\n",
    "\n",
    "1. Question 3(a): Assume the sample is representative of the population from which it was drawn. To generate new, synthetic samples, we resample with replacement from the original sample. Generate `B` resamples.\n",
    "2. Question 3(b): Calculate a statistic for each of the `B` resamples. Possible statistics are the mean (Lab 9), correlation (Lab 9), coefficients of a linear model (Question 3 below), or any function of the sample data.\n",
    "3. Question 3(c): Construct a confidence interval by grabbing relevant percentiles of the `B` synthetic summary statistics. For example, for an 80% confidence interval, we grab the 10th and 90th percentiles.\n",
    "4. Question 3(d): Use the resulting confidence interval to make inferences about the population-level statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b26f95",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Fill in the blanks below to implement the `bootstrap_sample()` function.\n",
    "\n",
    "Specification:\n",
    "- `bootstrap_sample()` returns a list of `B` bootstrap `DataFrame`s of the dataset `data`. \n",
    "- To generate a bootstrapped dataset from an actual dataset with $n$ rows, sample $n$ rows uniformly at random *with replacement* so that the bootstrapped `DataFrame` is the same size as the original `DataFrame`.\n",
    "\n",
    "**Hint**: You may find `df.sample()` helpful, see [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html). This is very similar to Lab 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d7436",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def bootstrap_sample(data, B):\n",
    "    \"\"\"\n",
    "    Performs bootstrap sampling on data to obtain B samples of size n.\n",
    "    \n",
    "    Arguments:\n",
    "        data - Dataset contained as a pandas DataFrame \n",
    "        B - Number of randomly drawn samples\n",
    "    \n",
    "    Returns:\n",
    "        samples - List containing B pandas DataFrames of size n each\n",
    "                  corresponding to each sample  \n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "# Print out the first DataFrame only\n",
    "bootstrap_sample(cleaned_college_data, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d04bfb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c545e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Write a new function `generate_models()` with the following specifications:\n",
    "\n",
    "1. Generate 1000 bootstrapped samples from an inputted `DataFrame` called `original_df`. `bootstrap_sample()` will come in handy.\n",
    "2. For each of the 1000 bootstrapped samples, use `sklearn` to fit a linear regression model (with an intercept term) like we did in Question 2. `Overall Score (0-100)` is the response, and the rest of the columns are features. You should fit 1000 models in total. \n",
    "3. Store each of the 1000 trained models in a list called `models`. Return `models`.\n",
    "\n",
    "**WARNING**: Do not use `X` and `Y` as variable names while bootstrapping, as this will override the values stored in `q2a`.\n",
    "    \n",
    "**Hints:**\n",
    "* You __should not__ create any validation or testing sets in this subpart; each model should fit to one entire bootstrapped `DataFrame`.\n",
    "* `LinearRegression` is an object type; to store a new model, you must create a new `LinearRegression` instance first!\n",
    "* When fitting each model, remember that your design matrix should be a 2D array or a `pandas` `DataFrame`, whereas the true labels should be a `Series`.\n",
    "\n",
    "**Note:** This question may take a few seconds to run due to the number of models being fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1b10b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(100) # DO NOT REMOVE THIS LINE\n",
    "\n",
    "def generate_models(original_df):\n",
    "    datasets = ...\n",
    "    models = []\n",
    "    ...\n",
    "    # Datasets take up a lot of memory, so we should remove them!\n",
    "    del datasets\n",
    "    return ...\n",
    "\n",
    "\n",
    "full_feature_models = generate_models(cleaned_college_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1abb6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754713cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "Complete the `confidence_interval` function below so that it generates a $95\\%$ confidence interval for each of our $p+1$ parameters in the original linear model, including the intercept term $\\theta_0$. \n",
    "\n",
    "Note: All of the helper code needed to extract coefficients from a list of trained models has been implemented for you.\n",
    "\n",
    "**Hint**: \n",
    "- For a refresher on confidence intervals, refer to this section in the [Data 8 textbook](https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html). \n",
    "- Pay close attention to how the arrays used below are formatted. What does each row represent? What does each column represent? To get the $i^{th}$ column from a 2D-array, you can use `2D_array[:, i]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b3e47",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_coefs(models, include_intercept = True):\n",
    "    \"\"\"\n",
    "    NOTE: This function has already been implemented. You do not need to modify this!\n",
    "    \n",
    "    Extracts coefficients of all the linear regression models in MODELS and returns\n",
    "    it as a NumPy array with each model's coefficients as one row.\n",
    "    \n",
    "    Arguments:\n",
    "        models - Contains k sklearn LinearRegression models, each with p + 1 coefficients.\n",
    "        include_intercept - Whether to include an intercept in returned coefficients.\n",
    "    \n",
    "    Returns:\n",
    "        coef_array - Coefficients of all k models, each with p + 1 coefficients (if intercept\n",
    "                     enabled, otherwise p). The returned object is k x (p + 1) NumPy array.\n",
    "    \"\"\"\n",
    "    coef_array = np.zeros(shape = (len(models), len(models[0].coef_) + 1))\n",
    "    for i, m in enumerate(models):\n",
    "        coef_array[i, 0] = m.intercept_\n",
    "        coef_array[i, 1:] = m.coef_\n",
    "    if include_intercept:\n",
    "        return coef_array \n",
    "    return coef_array[:, 1:]\n",
    "\n",
    "def confidence_interval(coefs):\n",
    "    \"\"\"\n",
    "    Calculates confidence intervals for each theta_i based on coefficients of \n",
    "    bootstrapped models. Returns output as a list of confidence intervals.\n",
    "    \n",
    "    Arguments:\n",
    "        coefs - Output of extract_coefs, a k x (p + 1) or k x p NumPy array containing\n",
    "                coefficients of bootstrapped models.\n",
    "    \n",
    "    Returns:\n",
    "        cis - Confidence intervals of each parameter theta_i in the form of a \n",
    "              list like this: [(0.5, 0.75), (0.2, 0.4), ...].\n",
    "    \"\"\"\n",
    "    cis = []\n",
    "    \n",
    "    # FILL IN CODE BELOW\n",
    "    for i in range(...):\n",
    "        theta_i_values = ...\n",
    "        theta_i_lower_ci, theta_i_upper_ci = np.percentile(...), np.percentile(...)\n",
    "        cis.append((theta_i_lower_ci, theta_i_upper_ci))\n",
    "    \n",
    "    return cis\n",
    "\n",
    "\n",
    "# Compute confidence intervals\n",
    "np.random.seed(100) # DO NOT REMOVE THIS LINE\n",
    "full_feature_models_coefs = extract_coefs(full_feature_models)\n",
    "full_feature_cis = confidence_interval(full_feature_models_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9c21a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472257c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3d\n",
    "Using the code you have written above, we'll now compute the confidence intervals for the coefficients of two different linear models: Model A and Model B. \n",
    "\n",
    "- Recall that Model A uses all features, as defined in `full_feature_models`.\n",
    "\n",
    "- Model B uses a subset of features. The code below will help you fit Model B's coefficients and compute confidence intervals for them. Instances of Model B will be contained in `partial_feature_models`:\n",
    "\n",
    "**Note**: Depending on your implementation of `generate_models` and whether you ignore particular columns in that function, you may need to change the input of `generate_models`. You can use the sanity check below to test whether your `partial_feature_models` were built with the correct features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c908ce2-3933-4a79-95d0-9a9ad5a1d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_features = [\n",
    "     \"Peer Assessment Score (1-5)\",\n",
    "     \"Acceptance rate\"\n",
    "]\n",
    "partial_feature_models = generate_models(\n",
    "    cleaned_college_data[[\"Overall Score (0-100)\"] + model_b_features]\n",
    ")\n",
    "partial_feature_models_coefs = extract_coefs(partial_feature_models)\n",
    "partial_feature_cis = confidence_interval(partial_feature_models_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ab748-c546-4b3c-8069-ffe8b425c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a sanity check, the following lines of code should all output True\n",
    "print(\"Coefficient shape is correct:\", partial_feature_models_coefs.shape == (1000, 3))\n",
    "print(\"Model input feature names are correct:\", (partial_feature_models[0].feature_names_in_ == model_b_features).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec811d-1a97-4f83-92d4-51b9d530c79c",
   "metadata": {},
   "source": [
    "Here's a function that allows us to inspect the confidence interval of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d070c4-3067-4daf-9c34-f7f613a095d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confidence_intervals(models, cis):\n",
    "    display(Markdown('##### Confidence Intervals:'))\n",
    "    md_list = [\"|parameter|feature name|lower|upper|\",\n",
    "               \"----|----|----|----|\"]\n",
    "    coef_names = np.append(['Intercept'], models[0].feature_names_in_)\n",
    "    md_list += [r\"|$\\theta_{\" + str(i) + \"}\" + fr\"$|{f_name}|{np.round(lci, 3)}|{np.round(uci, 3)}|\" for i, ((lci, uci), f_name) in enumerate(zip(cis, coef_names))]\n",
    "    display(Markdown('\\n'.join(md_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ed4f9-75cd-4f08-be75-dcdb3972b531",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 3d, Part i\n",
    "Let us first interpret **Model B**, the linear regression model that uses a subset of features from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6af13-738d-4d19-82a6-54c642eff7c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Model B: Subset of Features'))\n",
    "print_confidence_intervals(partial_feature_models, partial_feature_cis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4886ad-4356-4ec5-8723-dc6a617492ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Are $\\theta_1$ and $\\theta_2$ significantly different from 0 at a 5% significance level? How do you know? \n",
    "\n",
    "Does your answer imply that the relationship between `Overall Score (0-100)`, `Peer Assessment Score (1-5)`, and `Acceptance rate` are causal? Do you think the relationships are causal? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9eebd",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae286b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 3d, Part ii\n",
    "\n",
    "In what situation(s) would you prefer a more compact model with just key features, like Model B? On the other hand, in what situation(s) would you want to consider many features, like in Model A? Explain your answer to both of these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ebce9",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390b64e-90ca-4608-b4b7-df14be823c2d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## ‚öñÔ∏è Question 4: Experimenting with Gradient Descent\n",
    "\n",
    "This question is a standalone question regarding Gradient Descent. In this question, you will experiment with the hyperparameters of gradient descent: \n",
    "- `alpha`: The learning rate, how far we move with each update\n",
    "- `num_updates`: The number of times the parameters $\\theta$ will be updated\n",
    "- `initial_theta`: Our initial guess for the values of the parameters $\\theta$\n",
    "- `batch_size`: The number of training examples used in one gradient descent update\n",
    "\n",
    "You will examine the effects of each hyperparameter using both a **convex** and a **nonconvex** objective function. \n",
    "\n",
    "Finally, you will compare **batch**, **mini-batch**, and **stochastic gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5fe0e-6f19-47e1-9889-63f49457638b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4a\n",
    "\n",
    "For this question, we have created some random features `X` and some target variable `y` such that the true relationship between `X` and `y` is:\n",
    "$$y = 3x_1 - 2 x_2 + \\epsilon$$\n",
    "\n",
    "where $x_1$ corresponds to the first column of `X`, $x_2$ corresponds to the second column of `X`, and $\\epsilon$ is a randomly drawn from distribution such that $\\mathbb{E}[\\epsilon]=0$.\n",
    "\n",
    "Based on the data-generating process (DGP) above, we might expect that the optimal parameters of OLS when fit to a large random sample of data with $p=2$ and no intercept would be close to $\\hat{\\theta}_1 = 3$ and $\\hat{\\theta}_2 = -2$. In this problem, we will observe how gradient descent can arrive at this solution. \n",
    "\n",
    "There are 1000 observations (rows) for `X` and `y`.\n",
    "\n",
    "Run the following cell to define our `X` and `y`. **DO NOT EDIT THIS CELL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088010c1-fa93-49b3-b37a-e2ba5daa227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL, but\n",
    "# DO NOT EDIT THIS CELL\n",
    "np.random.seed(42)\n",
    "X = generate_X()\n",
    "y = generate_Y(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86de96-3121-45b5-b39f-0f1e2f0a2d34",
   "metadata": {},
   "source": [
    "In reality, we often don't know the relationship between our `X` and `y` values. One way to estimate $\\theta_1$ and $\\theta_2$ is by fitting a linear model with $p=2$ and no intercept, where loss is defined by some objective function. To fit the model, we can use gradient descent to identify the values of $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ that minimize the objective function.\n",
    "\n",
    "In this question we will use 2 objective functions:\n",
    "\n",
    "1. A **convex objective function** that has one global minimum at $[3, -2]$ for our `X` and `y` data.\n",
    "2. A **nonconvex objective function** that has both a global minimum at $[3, -2]$ and a local minimum elsewhere for `X` and `y` data.\n",
    "\n",
    "Run the following cell to visualize both functions in an interactive 3D plot and a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89587d2e-6460-40a1-8629-b31e00b33685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL, but\n",
    "# DO NOT EDIT THIS CELL\n",
    "plot_function(\"Convex Function\", X, y, convex = True)\n",
    "plot_function(\"Nonconvex Function\", X, y, convex = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1610b-2e1c-462c-a2da-de0b7e7b6e28",
   "metadata": {},
   "source": [
    "The following cell performs the gradient descent algorithm:\n",
    "\n",
    "$$\n",
    "    \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla_\\theta L(\\theta^{(t)})$$\n",
    "\n",
    "and creates a visualization that shows us each step of gradient descent. Check out [lecture 14](https://ds100.org/fa25/lectures/14/) if you need a refresher on the gradient descent algorithm.\n",
    "\n",
    "In the following cell, we have set the hyperparameters of gradient descent as follows:\n",
    "- `initial_theta` = $[0,0]$\n",
    "- `alpha` = $1$\n",
    "- `num_updates` = $5$\n",
    "- `batch_size` = `len(X)`\n",
    "\n",
    "We have also set `convex` = `True` which means we are applying gradient descent to the **convex** objective function. Setting this to `False` will apply gradient descent to the **nonconvex** objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc55fe2-deaf-4350-a58d-7c8f1d6d0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL, but\n",
    "# DO NOT EDIT THIS CELL\n",
    "visualize_gradient_descent(\n",
    "    title = \"Gradient Descent Visualization\", \n",
    "    X = X, \n",
    "    y = y, \n",
    "    initial_theta = [0,0], \n",
    "    alpha = 1, \n",
    "    num_updates = 5, \n",
    "    batch_size = len(X),\n",
    "    convex = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29492c1-ad7b-46db-8588-0b5865f4c296",
   "metadata": {},
   "source": [
    "For these set of hyperparameters, the gradient descent algorithm converges to the minimum of the convex objective function. \n",
    "\n",
    "Now it's your turn to experiment with the hyperparameters of gradient descent. See what happens when you change the hyperparameters or when you use the nonconvex objective function instead of the convex function. Then answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c73d0-3b99-49c8-a9c4-653f978d6150",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Fill in the hyperparameters to visualize gradient descent\n",
    "# Consider adding a title to describe what is in each plot you produce\n",
    "visualize_gradient_descent(\n",
    "    title = ...\n",
    "    X = ...\n",
    "    y = ...\n",
    "    initial_theta = ...\n",
    "    alpha = ...\n",
    "    num_updates = ...\n",
    "    batch_size = ...\n",
    "    convex = ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f4158-9073-4929-8f48-a00c1cc049b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 4a, Part i\n",
    "Describe what happens when you increase or decrease the value of `alpha`. Why does this happen?\n",
    "\n",
    "**Hint**: test `alpha` = $0.1, 1, 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0a04c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a256751-f561-4c60-9ca4-dc98a7f97799",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 4a, Part ii\n",
    "Describe what happens when you change the `initial_theta` for the convex function? What about for the nonconvex function? What happens to the optimal parameters that gradient descent converges to and why?\n",
    "\n",
    "**Hint**: for the **nonconvex objective function**, try `initial_theta` = $[-3, 7]$ and $[3, 7]$. You may need to play around with `alpha` and `num_updates` to determine where it is converging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9bd7c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c445a43-7309-47ed-b51a-a57b1da29890",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 4a, Part iii\n",
    "What happens when we change the value of `num_updates`? How can we change both `alpha` and `num_updates` to increase our chances of always converging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338024d1",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fca88d-df02-4690-95cc-41413289369f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "Next, we are going to compare the results of running batch, mini-batch, and stochastic gradient descent:\n",
    "- **Batch gradient descent** includes all 1000 rows of `X` in each gradient descent update.\n",
    "- **Mini-batch gradient descent** includes a sample of rows of `X` in each gradient descent update (e.g., 100 rows per update).\n",
    "- **Stochastic gradient descent** will perform a gradient descent update for every single row in `X`.\n",
    "\n",
    "Check out the [lecture slides](https://docs.google.com/presentation/d/1_ieFeZ-UVUkXiavI0ZTya2tNSg41ZWe-_ZqH2G0Zy_Q/edit?slide=id.g33c70fcbe44_0_612#slide=id.g33c70fcbe44_0_612) if you need a reminder on properties of each type of gradient descent algorithm.\n",
    "                                                                                                                             \n",
    "In the 3 cells below adjust the hyperparemeters to visualize **1 epoch** of **batch gradient descent**, **mini-batch gradient descent** with a batch size of 100, and **stochastic gradient descent**.\n",
    "                                                                                                                             \n",
    "Once again, you can experiment with the other hyperparameters of gradient descent and see what happens when you use the the nonconvex objective function instead of the convex function. Then answer the questions below.\n",
    "                                                                                                                             \n",
    "**Note**: You will not be graded on the visualizations, but you will need to make these visualizations in order to answer the question below.\n",
    "                                                                                                                             \n",
    "**Hint 1**: Remember that an epoch is defined as a single gradient computation of the loss for each of the n rows in the dataset. There are 1000 observations in the dataset.\\\n",
    "**Hint 2**: Consider what the value of `num_updates` should be for 1 epoch for each type of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e31ba-473c-4420-a08b-ccdf3ef6ff7a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "visualize_gradient_descent(\n",
    "    title = \"Batch gradient descent\",\n",
    "    X = ...\n",
    "    y = ...\n",
    "    initial_theta = ...\n",
    "    alpha = ...\n",
    "    num_updates = ...\n",
    "    batch_size = ...\n",
    "    convex = ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda51c51-1bf0-467e-9890-5e62a1d61fc7",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent with batch size of 100\n",
    "visualize_gradient_descent(\n",
    "    title = \"Mini-batch gradient descent with batch size of 100\",\n",
    "    X = ...\n",
    "    y = ...\n",
    "    initial_theta = ...\n",
    "    alpha = ...\n",
    "    num_updates = ...\n",
    "    batch_size = ...\n",
    "    convex = ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3f7e0-2941-41b4-b274-9412ee42ae80",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "visualize_gradient_descent(\n",
    "    title = \"Stochastic gradient descent\",\n",
    "    X = ...\n",
    "    y = ...\n",
    "    initial_theta = ...\n",
    "    alpha = ...\n",
    "    num_updates = ...\n",
    "    batch_size = ...\n",
    "    convex = ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fed5e-b0ea-4d74-adeb-61eb1f05c387",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Which of the three types of gradient descent tends to reach a local minimum in the fewest epochs? \\\n",
    "**A**: Batch gradient descent\\\n",
    "**B**: Mini-batch gradient descent\\\n",
    "**C**: Stochastic gradient descent\n",
    "\n",
    "Justify your answer with reference to your visualizations. Write you answer in 2-3 sentences.\n",
    "\n",
    "**Hint**: Epochs can be fractional. For example, 0.1 epochs is defined as computing the gradient of loss on 10% of the data.\n",
    "\n",
    "**Reminder**: You will not be graded on the visualizations you make. You will only be graded on your letter answer and written justification to this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3bb1f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20bd06-2331-4537-8fe6-9bf92cc8cfcc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## ‚öñÔ∏è Question 5: Performing Bias-Variance Analysis (Optional)\n",
    "\n",
    "This question is an **optional** question that continues from the work we did in Q1-Q3. This question will guide you through the concepts of model bias and model variance.\n",
    "\n",
    "Recall that the **model variance** on a particular data point ($\\vec{x_k}$) is the variance of the model predictions for $\\vec{x_k}$ across parallel universes of randomly sampled training datasets:\n",
    "\n",
    "$$\\text{model variance} = \\mathrm{Var}(\\hat{Y}(\\vec{x_k})) = \\mathrm{Var}(\\vec{x_k}^T\\hat{\\theta})$$\n",
    "\n",
    "> Note that $\\hat{Y}(\\vec{x_k})$ is an alternative way to write $\\hat{f}(\\vec{x_k})$ from lecture.\n",
    "\n",
    "To estimate the model variance, we can sample a particular data point $(\\vec{x_k}, y_k)$ and calculate the variance of the predictions for $(\\vec{x_k}, y_k)$ across parallel sampling universes, where each universe has a model fit to a randomly sampled training dataset. \n",
    "\n",
    "- As it turns out, you already have these fitted models stored in `models`!\n",
    "\n",
    "-  Note that the variance is computed across the different models, which have the same shared structure (i.e., the same input features and transformations). However, fitted parameter values will differ across models since the training data differs across models.\n",
    "\n",
    "Recall that **model risk** for a single point is the same as the mean squared error (MSE) of $\\vec{x_k}$ over our parallel universes of fitted models:\n",
    "\n",
    "$$\n",
    "\\text{model risk} = \\mathbb{E}\\left[\\left(y_k - \\hat{Y}(\\vec{x_k}) \\right)^2\\right] \\approx \\frac{1}{\\# \\text{ of bootstrap}}\\sum_{j=1}^{\\# \\text{ of bootstrap}} (y_k - \\hat{Y}_j(\\vec{x_k}))^2 = MSE(\\vec{x_k})\n",
    "$$\n",
    "\n",
    "where the subscript $j$ is used to index the different models. \n",
    "\n",
    "We can also compute the ratio of model variance to model risk:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{model variance}}{\\text{model risk}}=\\frac{\\mathrm{Var}(\\hat{Y}(\\vec{x_k}))}{\\mathbb{E}\\left[\\left(y_k - \\hat{Y}(\\vec{x_k}) \\right)^2\\right]}\n",
    "$$\n",
    "\n",
    "This ratio provides a sense of how much of model risk can be attributed to model variance, relative to squared bias and irreducible error ($\\sigma^2$). You may find it helpful to recall the bias-variance decomposition from lecture:\n",
    "\n",
    "$$\n",
    "\\text{model risk} = \\sigma^2 + (\\text{model bias})^2 + \\text{model variance}\n",
    "$$\n",
    "\n",
    "where $\\sigma^2$ is the observational variance, or \"irreducible error\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302f9fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5a (Optional)\n",
    "\n",
    "The function `simulate()` below takes in a single data point `xk`, `yk` and a list of models `models`.\n",
    "\n",
    "- `xk` is a list of feature values for a single data point. `yk` is the scalar outcome.\n",
    "\n",
    "`simulate()` function should return three quantities: the estimated model risk (`model_risk`), the estimated model variance (`model_var`), and the ratio of model variance to model risk (`ratio`).\n",
    "\n",
    "We have left one line blank for you to fill in:\n",
    "\n",
    "```model_risk, model_var, ratio = ...```\n",
    "\n",
    "Fill in the line correctly to complete the function `simulate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c888da",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def simulate(xk, yk, models):\n",
    "    mystery_a = [model.predict([xk])[0] for model in models]\n",
    "    mystery_b = np.var(mystery_a)\n",
    "    mystery_c = np.mean((mystery_a - yk) ** 2)\n",
    "    mystery_d = mystery_b / mystery_c\n",
    "    model_risk, model_var, ratio = ...\n",
    "    return model_risk, model_var, ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2474933",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5b (Optional)\n",
    "\n",
    "Using the `simulate` function from above, we can compute the model risk, model variance, and variance-to-risk ratio of **Model B**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7146",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "x_last = X_original.iloc[X_original.shape[0] - 100]\n",
    "y_last = Y_original.iloc[X_original.shape[0] - 100]\n",
    "\n",
    "(\n",
    "    partial_feature_model_risk,\n",
    "    partial_feature_model_var,\n",
    "    partial_feature_model_ratio\n",
    ") = simulate(x_last[model_b_features], y_last, partial_feature_models)\n",
    "\n",
    "print('Model B risk:')\n",
    "print(partial_feature_model_risk)\n",
    "\n",
    "print('Model B variance:')\n",
    "print(partial_feature_model_var)\n",
    "\n",
    "print('Model B ratio:')\n",
    "print(partial_feature_model_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa92f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Comment on the variance-to-risk ratio for Model B (`partial_feature_ratio`).\n",
    "\n",
    "- Does the model variance appear to be the dominant term in the bias-variance decomposition? If not, what term(s) dominate the bias-variance decomposition?\n",
    "\n",
    "Then, given your conclusion above, describe what operation(s) you might perform to reduce the model risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5feb6c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c0b07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You have finished Homework 6!\n",
    "\n",
    "## Data 100 course staff pets dressed up for Halloween!\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src = \"images/pet1.jpg\" width = \"400\" class=\"center\"> </td>\n",
    "<td> <img src = \"images/pet2.jpg\" width = \"400\" class=\"center\"> </td>\n",
    "<td> <img src = \"images/pet3.jpg\" width = \"300\" class=\"center\"> </td>\n",
    "<td> <img src = \"images/pet4.jpg\" width = \"400\" class=\"center\"> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "\n",
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://docs.google.com/forms/d/e/1FAIpQLScGQhiTz1qn5gsyYUu1Be5Yz0Z_kplIRR_os8UGMOAO8cc-yQ/viewform?usp=dialog). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the Homework 6 Coding assignment on Pensieve, Pensieve will automatically submit a PDF file with your written answers to the Homework 6 Written assignment. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide.\n",
    "\n",
    "**Important**: Please check that your **plots/graphs and written responses** were generated and submitted correctly to the Homework 6 Written Assignment.\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that the PDF for Homework 6 written answers was generated/submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline. \n",
    "\n",
    "You are responsible for submitting a PDF with all your answers to the Homework 6 Probability questions to **Homework 6 Probability** assignment on Pensieve. You **must tag pages to each question correctly** (it prompts you to do this after submitting your work). Failure to do this may result in a score of 0 for untagged questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfd52c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3014298",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f1f39",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cleaned_college_data.shape == (167, 22)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'Pell gradrate_missing' in cleaned_college_data.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> _unknown_test_1a = college_data.copy()\n>>> for _col in college_data.columns:\n...     if any(_unknown_test_1a[_col].isna()):\n...         _unknown_test_1a[_col + '_missing'] = _unknown_test_1a[_col].isna()\n>>> _test_1a = ((cleaned_college_data == _unknown_test_1a) | _unknown_test_1a.isna()).all().all()\n>>> del _unknown_test_1a\n>>> bool(_test_1a)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(cleaned_college_data.dtypes.iloc[1:].apply(lambda t: pd.api.types.is_numeric_dtype(t)).all())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> cleaned_college_data.shape == (167, 22)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.isclose(cleaned_college_data.iloc[:, 1:].sum().values.sum(), 131212.00080898977, rtol=10.0))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(1.3 <= train_rmse_cpc <= 1.32)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(2.14 <= test_rmse_cpc <= 2.16)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(all([len(df_i) == len(cleaned_college_data) for df_i in bootstrap_sample(cleaned_college_data, 1)]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(len(bootstrap_sample(cleaned_college_data, 3)) == 3)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(len(full_feature_models) == 1000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(X.equals(cleaned_college_data.iloc[:, 1:]) and Y.equals(cleaned_college_data.iloc[:, 0]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(all([isinstance(model, lm.LinearRegression) for model in full_feature_models]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(len(full_feature_cis) == 21 and all((len(ci) == 2 for ci in full_feature_cis)))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
